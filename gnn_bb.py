# -*- coding: utf-8 -*-
"""GNN_BB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x5oAArg-L_62kMe91GJ0SWYRyKf4vq9o
"""

import os, json
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from tqdm import tqdm

import torch
import torch.nn.functional as F
from torch import nn, optim
from torch_geometric.data import Data, DataLoader, InMemoryDataset
from torch_geometric.nn import SAGEConv

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)

import json

causal_graph = {
  "nodes": [
    {"id": "orders_cpu", "label": "orders_cpu", "in_degree": 2, "out_degree": 0, "is_root_cause": False},
    {"id": "orders_memory", "label": "orders_memory", "in_degree": 0, "out_degree": 1, "is_root_cause": True},
    {"id": "payment_latency", "label": "payment_latency", "in_degree": 1, "out_degree": 1, "is_root_cause": False},
    {"id": "trace_latency", "label": "trace_latency", "in_degree": 2, "out_degree": 1, "is_root_cause": False},
    {"id": "payment_errors", "label": "payment_errors", "in_degree": 0, "out_degree": 1, "is_root_cause": True},
    {"id": "error_count", "label": "error_count", "in_degree": 0, "out_degree": 1, "is_root_cause": True},
    {"id": "deployment_events", "label": "deployment_events", "in_degree": 0, "out_degree": 0, "is_root_cause": True}
  ],
  "edges": [
    {"source": "orders_memory", "target": "orders_cpu", "confidence": 1.0},
    {"source": "payment_latency", "target": "orders_cpu", "confidence": 1.0},
    {"source": "trace_latency", "target": "payment_latency", "confidence": 1.0},
    {"source": "payment_errors", "target": "trace_latency", "confidence": 0.8333333333333334},
    {"source": "error_count", "target": "trace_latency", "confidence": 1.0}
  ],
  "metadata": {"total_nodes": 7, "total_edges": 5, "is_dag": True}
}

with open('/content/causal_graph.json', 'w') as f:
    json.dump(causal_graph, f, indent=2)

print("✅ causal_graph.json file created successfully!")

# run after building `timestamps` and before creating datasets
import numpy as np

# labels_df must exist and have timestamp parsed as datetime
if labels_df is None:
    print("No labels available at all.")
else:
    # map timestamp -> has_positive? and positive count
    pos_per_ts = labels_df.groupby('timestamp')['label'].agg(['sum','count']).reset_index()
    pos_map = {r['timestamp']: int(r['sum']) for _, r in pos_per_ts.iterrows()}

    def pos_count_for_ts(ts):
        return pos_map.get(ts, 0)

    # compute counts across splits
    n = len(timestamps)
    t0 = 0
    t1 = int(0.7 * n)
    t2 = int(0.85 * n)

    splits = {
        'train': timestamps[t0:t1],
        'val': timestamps[t1:t2],
        'test': timestamps[t2:]
    }

    for name, ts_list in splits.items():
        total_labels = 0
        positives = 0
        for ts in ts_list:
            c = pos_count_for_ts(ts)
            positives += c
            # total label rows for that timestamp (approx N_nodes)
            total_labels += len(nodes)  # approximate; more accurate below if needed
        print(f"{name.upper():6s}: timestamps={len(ts_list):4d}  positives={positives:4d}")

# ==================================================
# 🚀 Generate synthetic node_metrics.csv + labels.csv
# ==================================================
import numpy as np, pandas as pd, random

# Use same nodes as in your causal graph
NODES = [n['id'] for n in causal_graph['nodes']]

NUM_TIMESTEPS = 500   # number of time windows
ANOMALY_PROB = 0.05   # 5% anomalies
NOISE_STD = 0.1       # random noise
timestamps = pd.date_range('2025-01-01', periods=NUM_TIMESTEPS, freq='5min')

base_profiles = {}
for node in NODES:
    base_profiles[node] = {
        'cpu': np.random.uniform(0.3, 0.7),
        'memory': np.random.uniform(0.4, 0.8),
        'latency': np.random.uniform(0.2, 0.6),
        'errors': np.random.uniform(0.0, 0.05)
    }

rows, labels = [], []
for ts in timestamps:
    anomaly_nodes = []
    if random.random() < ANOMALY_PROB:
        anomaly_nodes = random.sample(NODES, random.randint(1,2))
    for node in NODES:
        base = base_profiles[node]
        cpu = base['cpu'] + np.random.normal(0, NOISE_STD)
        mem = base['memory'] + np.random.normal(0, NOISE_STD)
        lat = base['latency'] + np.random.normal(0, NOISE_STD)
        err = base['errors'] + np.random.normal(0, NOISE_STD)
        if node in anomaly_nodes:
            cpu *= np.random.uniform(1.5, 3.0)
            mem *= np.random.uniform(1.5, 3.0)
            lat *= np.random.uniform(2.0, 5.0)
            err += np.random.uniform(0.1, 0.5)
        cpu, mem, lat, err = np.clip([cpu, mem, lat, err], 0, 1)
        rows.append([ts, node, cpu, mem, lat, err])
        labels.append([ts, node, 1 if node in anomaly_nodes else 0])

metrics_df = pd.DataFrame(rows, columns=['timestamp','node','cpu','memory','latency','errors'])
labels_df = pd.DataFrame(labels, columns=['timestamp','node','label'])

metrics_df = metrics_df.drop(metrics_df.sample(frac=0.05).index)  # simulate missing data

metrics_df.to_csv('/content/node_metrics.csv', index=False)
labels_df.to_csv('/content/labels.csv', index=False)

print("✅ Created synthetic /content/node_metrics.csv and /content/labels.csv")
print(metrics_df.head())
print(f"Total rows: {len(metrics_df)}, anomalies: {labels_df['label'].sum()} ({labels_df['label'].mean()*100:.2f}%)")

# =======================================
# 3️⃣ Load causal graph (Grafana JSON)
# =======================================
DATA_DIR = '/content'   # or your Google Drive path
graph_path = os.path.join(DATA_DIR, 'causal_graph.json')
metrics_path = os.path.join(DATA_DIR, 'node_metrics.csv')

assert os.path.exists(graph_path), "Upload your causal_graph.json first!"
assert os.path.exists(metrics_path), "Upload your node_metrics.csv!"

with open(graph_path, 'r') as f:
    causal_graph = json.load(f)

nodes_json = causal_graph['nodes']
edges_json = causal_graph['edges']

# Extract node IDs
nodes = [n['id'] for n in nodes_json]
node2idx = {n: i for i, n in enumerate(nodes)}
idx2node = {i: n for n, i in node2idx.items()}
N_nodes = len(nodes)

# Extract edge list + weights
edge_index = []
edge_weight = []
for e in edges_json:
    s, t = node2idx[e['source']], node2idx[e['target']]
    edge_index.append([s, t])
    edge_weight.append(float(e.get('confidence', 1.0)))

edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()
edge_weight = torch.tensor(edge_weight, dtype=torch.float)

print(f"Loaded graph with {N_nodes} nodes, {edge_index.shape[1]} edges")

# ===============================================
# 🔥 Realistic Imperfect Synthetic Dataset Generator
# ===============================================
import numpy as np, pandas as pd, random, networkx as nx

# Use same nodes from causal graph
NODES = [n['id'] for n in causal_graph['nodes']]
EDGES = [(e['source'], e['target']) for e in causal_graph['edges']]

G = nx.DiGraph()
G.add_nodes_from(NODES)
G.add_edges_from(EDGES)

NUM_TIMESTEPS = 1000
BASE_NOISE = 0.15         # baseline metric noise
DRIFT_PROB = 0.1          # how often baseline drifts
CASCADE_PROB = 0.25       # probability of cascading anomalies
MISSING_FRAC = 0.05       # % of missing rows
FALSE_LABEL_FRAC = 0.05   # % of wrong labels
timestamps = pd.date_range('2025-01-01', periods=NUM_TIMESTEPS, freq='5min')

# --- baseline profile ---
base_profiles = {n: {
    'cpu': np.random.uniform(0.3, 0.7),
    'memory': np.random.uniform(0.4, 0.8),
    'latency': np.random.uniform(0.2, 0.6),
    'errors': np.random.uniform(0.0, 0.05)
} for n in NODES}

rows, labels = [], []

for t_idx, ts in enumerate(timestamps):
    anomaly_nodes = []
    # random baseline drift (like load spike)
    if random.random() < DRIFT_PROB:
        node = random.choice(NODES)
        for k in base_profiles[node]:
            base_profiles[node][k] = np.clip(base_profiles[node][k] + np.random.uniform(-0.05, 0.05), 0, 1)

    # randomly start an anomaly
    if random.random() < 0.08:
        seed_node = random.choice(NODES)
        anomaly_nodes.append(seed_node)

        # cascading anomalies based on causal graph
        if random.random() < CASCADE_PROB:
            for down in nx.descendants(G, seed_node):
                if random.random() < 0.5:
                    anomaly_nodes.append(down)

    # Generate metrics for each node
    for node in NODES:
        base = base_profiles[node]
        cpu = base['cpu'] + np.random.normal(0, BASE_NOISE)
        mem = base['memory'] + np.random.normal(0, BASE_NOISE)
        lat = base['latency'] + np.random.normal(0, BASE_NOISE)
        err = base['errors'] + np.random.normal(0, BASE_NOISE / 2)

        if node in anomaly_nodes:
            cpu *= np.random.uniform(1.3, 3.5)
            mem *= np.random.uniform(1.2, 2.5)
            lat *= np.random.uniform(2.0, 6.0)
            err += np.random.uniform(0.1, 0.5)

        # Simulate delayed propagation — downstream affected a few steps later
        if t_idx > 2 and random.random() < 0.2:
            parents = list(G.predecessors(node))
            if parents and any(p in anomaly_nodes for p in parents):
                lat *= np.random.uniform(1.3, 2.5)
                err += np.random.uniform(0.05, 0.2)

        # clamp values
        cpu, mem, lat, err = np.clip([cpu, mem, lat, err], 0, 1)
        rows.append([ts, node, cpu, mem, lat, err])
        labels.append([ts, node, 1 if node in anomaly_nodes else 0])

metrics_df = pd.DataFrame(rows, columns=['timestamp','node','cpu','memory','latency','errors'])
labels_df = pd.DataFrame(labels, columns=['timestamp','node','label'])

# --- add missing values (simulate scrape failures) ---
metrics_df = metrics_df.drop(metrics_df.sample(frac=MISSING_FRAC).index)

# --- introduce false positives/negatives ---
flip_idx = labels_df.sample(frac=FALSE_LABEL_FRAC).index
labels_df.loc[flip_idx, 'label'] = 1 - labels_df.loc[flip_idx, 'label']

# --- save ---
metrics_df.to_csv('/content/node_metrics.csv', index=False)
labels_df.to_csv('/content/labels.csv', index=False)

print("✅ Generated realistic node_metrics.csv and labels.csv")
print(metrics_df.head())
print(f"Total rows: {len(metrics_df)} | Anomalies: {labels_df['label'].sum()} ({labels_df['label'].mean()*100:.2f}%)")

metrics_df = pd.read_csv(metrics_path)
print(metrics_df.head())

# Parse timestamp
metrics_df['timestamp'] = pd.to_datetime(metrics_df['timestamp'])
metrics_df = metrics_df.sort_values('timestamp')

# Ensure all nodes in telemetry are known from causal graph
missing_nodes = set(metrics_df['node'].unique()) - set(nodes)
if missing_nodes:
    print("⚠️ New nodes not in causal graph, adding them:", missing_nodes)
    for n in missing_nodes:
        node2idx[n] = len(node2idx)
        idx2node[len(node2idx)-1] = n
    N_nodes = len(node2idx)

feature_cols = [c for c in metrics_df.columns if c not in ('timestamp', 'node')]
print("Feature columns:", feature_cols)

grouped = metrics_df.groupby('timestamp')
timestamps = sorted(grouped.groups.keys())

# Normalize all numeric features
scaler = StandardScaler()
all_feats = []
for ts, g in grouped:
    all_feats.append(g[feature_cols].values)
scaler.fit(np.vstack(all_feats))

def get_node_features(ts):
    """Return NxF matrix for timestamp ts."""
    g = grouped.get_group(ts)
    arr = np.zeros((N_nodes, len(feature_cols)))
    for _, r in g.iterrows():
        arr[node2idx[r['node']], :] = r[feature_cols].values
    arr = scaler.transform(arr)
    return torch.tensor(arr, dtype=torch.float)

# Optionally load labels if you have them
labels_path = os.path.join(DATA_DIR, 'labels.csv')
labels_df = pd.read_csv(labels_path) if os.path.exists(labels_path) else None

import networkx as nx
import random
import numpy as np
import pandas as pd

# Build the graph (safe version)
G = nx.DiGraph()
edges = [(e['source'], e['target']) for e in causal_graph['edges']]
G.add_nodes_from([n['id'] for n in causal_graph['nodes']])  # <-- ensures all nodes present
G.add_edges_from(edges)

# Parameters
ANOMALY_RATIO = 0.08
CASCADE_PROB = 0.4

timestamps = sorted(metrics_df['timestamp'].unique())
nodes = sorted(metrics_df['node'].unique())

new_labels = []

for ts in timestamps:
    if random.random() < ANOMALY_RATIO:
        seed_nodes = random.sample(nodes, random.randint(1, 2))
        anomaly_nodes = set(seed_nodes)
        for node in seed_nodes:
            # descendants() now always safe even if node not in any edge
            for downstream in nx.descendants(G, node):
                if random.random() < CASCADE_PROB:
                    anomaly_nodes.add(downstream)
        # apply corruption
        for node in anomaly_nodes:
            mask = (metrics_df['timestamp'] == ts) & (metrics_df['node'] == node)
            if mask.any():
                metrics_df.loc[mask, 'latency'] = np.clip(metrics_df.loc[mask, 'latency'] * np.random.uniform(2,5), 0, 1)
                metrics_df.loc[mask, 'errors'] = np.clip(metrics_df.loc[mask, 'errors'] + np.random.uniform(0.1,0.4), 0, 1)
        # add labels
        for node in nodes:
            new_labels.append({'timestamp': ts, 'node': node, 'label': 1 if node in anomaly_nodes else 0})
    else:
        for node in nodes:
            new_labels.append({'timestamp': ts, 'node': node, 'label': 0})

labels_df = pd.DataFrame(new_labels)
print("✅ Injected anomalies safely. Total:", labels_df['label'].sum(), "positive samples.")

# =======================================
# 6️⃣ Build PyTorch Geometric dataset
# =======================================
class TemporalDataset(InMemoryDataset):
    def __init__(self, timestamps):
        data_list = []
        for ts in timestamps:
            x = get_node_features(ts)
            y = None
            if labels_df is not None:
                lab = np.zeros((N_nodes,), dtype=int)
                subset = labels_df[labels_df['timestamp'] == ts]
                for _, r in subset.iterrows():
                    if r['node'] in node2idx:
                        lab[node2idx[r['node']]] = int(r['label'])
                y = torch.tensor(lab, dtype=torch.long)
            data = Data(x=x, edge_index=edge_index, edge_attr=edge_weight, y=y)
            data_list.append(data)
        super().__init__('/tmp')
        self.data, self.slices = self.collate(data_list)

timestamps_train = timestamps[:int(0.7*len(timestamps))]
timestamps_val = timestamps[int(0.7*len(timestamps)):int(0.85*len(timestamps))]
timestamps_test = timestamps[int(0.85*len(timestamps)):]

train_ds = TemporalDataset(timestamps_train)
val_ds   = TemporalDataset(timestamps_val)
test_ds  = TemporalDataset(timestamps_test)

train_loader = DataLoader(train_ds, batch_size=1, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=1)
test_loader  = DataLoader(test_ds, batch_size=1)

print("Datasets:", len(train_ds), len(val_ds), len(test_ds))

# =======================================
# 7️⃣ Define GraphSAGE model
# =======================================
class GraphSAGEAnomaly(nn.Module):
    def __init__(self, in_feats, hidden=64, dropout=0.3):
        super().__init__()
        self.conv1 = SAGEConv(in_feats, hidden)
        self.conv2 = SAGEConv(hidden, hidden)
        self.lin = nn.Linear(hidden, 2)
        self.dropout = dropout

    def forward(self, x, edge_index):
        h = F.relu(self.conv1(x, edge_index))
        h = F.dropout(h, p=self.dropout, training=self.training)
        h = F.relu(self.conv2(h, edge_index))
        return self.lin(h)

F_in = len(feature_cols)
model = GraphSAGEAnomaly(F_in, hidden=128).to(device)
optimizer = optim.Adam(model.parameters(), lr=1e-3)
print(model)

# =======================================
# 8️⃣ Enhanced training & evaluation
# =======================================
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import matplotlib.pyplot as plt

def train_epoch(loader):
    model.train()
    total_loss, total_samples = 0, 0
    for data in loader:
        data = data.to(device)
        if data.y is None:
            continue
        optimizer.zero_grad()
        out = model(data.x, data.edge_index)
        loss = F.cross_entropy(out, data.y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        total_samples += 1
    return total_loss / max(total_samples, 1)


@torch.no_grad()
def evaluate(loader):
    model.eval()
    y_true, y_pred, y_prob = [], [], []
    total_loss, total_samples = 0, 0

    for data in loader:
        data = data.to(device)
        if data.y is None:
            continue
        out = model(data.x, data.edge_index)
        loss = F.cross_entropy(out, data.y)
        total_loss += loss.item()
        total_samples += 1
        probs = F.softmax(out, dim=1)[:,1].detach().cpu().numpy()
        preds = out.argmax(dim=1).cpu().numpy()
        y = data.y.cpu().numpy()

        y_true.append(y)
        y_pred.append(preds)
        y_prob.append(probs)

    if not y_true:
        return {m: 0 for m in ['loss','acc','prec','rec','f1','auc']}

    y_true = np.concatenate(y_true)
    y_pred = np.concatenate(y_pred)
    y_prob = np.concatenate(y_prob)

    return {
        'loss': total_loss / max(total_samples, 1),
        'acc': accuracy_score(y_true, y_pred),
        'prec': precision_score(y_true, y_pred, zero_division=0),
        'rec': recall_score(y_true, y_pred, zero_division=0),
        'f1': f1_score(y_true, y_pred, zero_division=0),
        'auc': roc_auc_score(y_true, y_prob) if len(np.unique(y_true)) > 1 else 0
    }


# ---------------------------
# Main training loop
# ---------------------------
EPOCHS = 10
history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_prec': [], 'val_rec': [], 'val_f1': [], 'val_auc': []}
best_f1 = 0

for epoch in range(1, EPOCHS+1):
    tr_loss = train_epoch(train_loader)
    val_metrics = evaluate(val_loader)

    if val_metrics['f1'] > best_f1:
        best_f1 = val_metrics['f1']
        torch.save(model.state_dict(), '/content/gnn_best.pt')

    history['train_loss'].append(tr_loss)
    for k in ['loss','acc','prec','rec','f1','auc']:
        history[f'val_{k}'].append(val_metrics[k])

    print(f"Epoch {epoch:02d} | "
          f"train_loss={tr_loss:.4f} | "
          f"val_loss={val_metrics['loss']:.4f} | "
          f"acc={val_metrics['acc']:.3f} | "
          f"prec={val_metrics['prec']:.3f} | "
          f"rec={val_metrics['rec']:.3f} | "
          f"f1={val_metrics['f1']:.3f} | "
          f"auc={val_metrics['auc']:.3f}")

    if val_metrics['f1'] > best_f1:
        best_f1 = val_metrics['f1']
        torch.save(model.state_dict(), '/content/gnn_best.pt')

print(f"\n✅ Training finished! Best F1: {best_f1:.3f}")
torch.save(model.state_dict(), '/content/gnn_best.pt')
print(f"\n✅ Model checkpoint saved at /content/gnn_best.pt (best_f1={best_f1:.4f})")

# =======================================
# 9️⃣ Plot training metrics
# =======================================
plt.figure(figsize=(10,6))
plt.plot(history['train_loss'], label='Train Loss', color='blue')
plt.plot(history['val_loss'], label='Val Loss', color='orange')
plt.title("Loss over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()
plt.show()

plt.figure(figsize=(10,6))
plt.plot(history['val_acc'], label='Accuracy')
plt.plot(history['val_prec'], label='Precision')
plt.plot(history['val_rec'], label='Recall')
plt.plot(history['val_f1'], label='F1-score')
plt.plot(history['val_auc'], label='ROC-AUC')
plt.title("Validation Metrics over Epochs")
plt.xlabel("Epoch")
plt.ylabel("Score")
plt.legend()
plt.show()

# ======================================
# 🔍 Evaluate on Test Set
# ======================================
model.load_state_dict(torch.load('/content/gnn_best.pt', map_location=device))
test_metrics = evaluate(test_loader)
print("\n📊 TEST SET METRICS:")
for k,v in test_metrics.items():
    print(f"{k.upper():8s}: {v:.4f}")

# =======================================
# 📈 Visualize anomaly probabilities over time (fixed)
# =======================================
model.eval()
all_scores = []

for ts in timestamps[-50:]:  # last 50 time steps
    feats = get_node_features(ts).to(device)
    out = model(feats, edge_index.to(device))
    # detach before converting to numpy
    probs = F.softmax(out, dim=1)[:, 1].detach().cpu().numpy()
    all_scores.append(probs)

all_scores = np.array(all_scores)  # [T, N]
plt.figure(figsize=(10,5))
plt.imshow(all_scores.T, aspect='auto', cmap='hot', interpolation='nearest')
plt.yticks(range(N_nodes), [idx2node[i] for i in range(N_nodes)])
plt.xlabel("Time step")
plt.ylabel("Node")
plt.title("Predicted anomaly probability per node over time")
plt.colorbar(label="Anomaly probability")
plt.show()



